# -*- coding: utf-8 -*-
"""yolo_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_R7p32NuAjPffsVNZAcUXrNW0mVUe0C0

# ***Yolo Model***
"""

!pip install -q inference-gpu[yolo-world]==0.9.13

import cv2
import supervision as sv
from inference.models import YOLOWorld
import IPython.display as ipd
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import HTML
import base64

model = YOLOWorld(model_id="yolo_world/l")

classes = [
    "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat", "traffic light",
    "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow",
    "elephant", "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee",
    "skis", "snowboard", "sports ball", "kite", "baseball bat", "baseball glove", "skateboard", "surfboard",
    "tennis racket", "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
    "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "couch",
    "potted plant", "bed", "dining table", "toilet", "tv", "laptop", "mouse", "remote", "keyboard", "cell phone",
    "microwave", "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear",
    "hair drier", "toothbrush", "forklift", "tractor", "helicopter", "tricycle", "wheelbarrow", "scooter",
    "wagon", "baby carriage", "stroller", "hoverboard", "unicycle", "traffic cone", "luggage", "computer",
    "keyboard", "printer", "desktop", "tower", "monitor", "screen", "desktop", "lamp", "lampshade",
    "flashlight", "flash", "torch", "street sign", "stoplight", "stoplight", "firetruck", "ambulance",
    "taxi", "police car", "police van", "police wagon", "police", "police vehicle", "garbage truck",
    "rubbish truck", "dustcart", "ac"
]

model.set_classes(classes)
BOUNDING_BOX_ANNOTATOR = sv.BoundingBoxAnnotator(thickness=2)
LABEL_ANNOTATOR = sv.LabelAnnotator(text_thickness=2, text_scale=1, text_color=sv.Color.BLACK)

def detect_objects_on_video(video_path, output_video_path):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
    detections_list = []
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        results = model.infer(frame)
        detections = sv.Detections.from_inference(results)
        num_detections = len(detections)
        print("Number of detections:", num_detections)
        object_counts = {}

        for detection in detections:
            class_name = detection[5]['class_name']
            if class_name in object_counts:
                object_counts[class_name] += 1
            else:
                object_counts[class_name] = 1
        print("Object Counts:", object_counts)
        annotated_frame = BOUNDING_BOX_ANNOTATOR.annotate(frame, detections)
        annotated_frame = LABEL_ANNOTATOR.annotate(annotated_frame, detections)
        out.write(annotated_frame)
        detections_list.append(object_counts)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    cap.release()
    out.release()
    cv2.destroyAllWindows()

input_video = "/content/input_video.mp4"

cap = cv2.VideoCapture(input_video)

frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

print(f"Number of frames: {frame_count}")
print(f"Frame width: {width}, Frame height: {height}")

cap = cv2.VideoCapture("/content/input_video.mp4")
frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

fig, axs = plt.subplots(5, 5, figsize=(30, 20))
axs = axs.flatten()

temp = 0
for frame in range(frame_count):
    ret, img = cap.read()
    if ret == False:
        break
    if frame % 20 == 0:
        axs[temp].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        axs[temp].set_title(f'Frame: {frame}')
        axs[temp].axis('off')
        temp += 1
        if temp >= 25:
            break

plt.tight_layout()
plt.show()
cap.release()

video_data = open(input_video, "rb").read()
width = 700

HTML("""
<video controls width="{width}">
<source src="data:video/mp4;base64,{0}" type="video/mp4">
</video>
""".format(base64.b64encode(video_data).decode(), width=width))

output_video_path = "/content/output_video.mp4"
detect_objects_on_video(input_video, output_video_path)

cap = cv2.VideoCapture("/content/output_video.mp4")
frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

fig, axs = plt.subplots(5, 5, figsize=(30, 20))
axs = axs.flatten()

temp = 0
for frame in range(frame_count):
    ret, img = cap.read()
    if ret == False:
        break
    if frame % 20 == 0:
        axs[temp].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        axs[temp].set_title(f'Frame: {frame}')
        axs[temp].axis('off')
        temp += 1
        if temp >= 25:
            break

plt.tight_layout()
plt.show()
cap.release()

import cv2
from google.colab.patches import cv2_imshow
cap = cv2.VideoCapture('/content/output_video.mp4')
while (cap.isOpened()):
    ret, frame = cap.read()
    if ret == True:
        cv2_imshow( frame)
        if cv2.waitKey(25) & 0xFF == ord('q'):
            break
    else:
        break
cap.release()
cv2.destroyAllWindows()